<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models">
  <meta name="keywords" content="Groma, Localized Visual Tokenization, MLLM, LMM, Multimodal Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
<!--</nav>-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Groma: Localized Visual Tokenization <br> for Grounding Multimodal Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Chuofan Ma<sup>1</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block"><a href="https://enjoyyi.github.io/">Yi Jiang</a><sup>2&dagger;</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Jiannan Wu<sup>1</sup>,&nbsp&nbsp&nbsp</span>
            </span><span class="author-block">Zehuan Yuan<sup>2</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block"><a href="https://xjqi.github.io/">Xiaojuan Qi</a><sup>1&dagger;</sup>,&nbsp&nbsp&nbsp</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Hong Kong,&nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>2</sup>ByteDance Inc.</span>
          </div>

          <div class="column has-text-centered">
<!--            <div class="publication-links">-->
<!--               PDF Link.-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.13013"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/FoundationVision/Groma"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Huggingface Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">&#129303</span>
                  <span>Demo (Comming Soon)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--          <img src="static/images/teaser.png" alt="Teaser image."/>-->
<!--          <h2 class="subtitle has-text-justified">-->
<!--            Groma is a multimodal large language model with exceptional region understanding and visual grounding capabilities.-->
<!--            It can take user-defined region inputs (boxes) as well as generate long-form responses that are grounded to visual context.-->
<!--          </h2>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability.
            Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding.
            Such capabilities are built upon a localized visual tokenization mechanism, where an image is decomposed into regions
            of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses,
            we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images.
            Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging
            the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization,
            Groma consistently demonstrates superior performances in standard referring and grounding benchmarks,
            highlighting the advantages of embedding localization into image tokenization.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
              <img src="static/images/teaser.jpg" alt="Teaser image."/>
              <h2 class="subtitle has-text-justified">
                Groma is a multimodal large language model with exceptional region understanding and visual grounding capabilities.
                It can take user-defined region inputs (boxes) as well as generate long-form responses that are grounded to visual context.
              </h2>
          </div>
        </div>
    </div>
  </div>
</section>

<br>
<br>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            <b>A novel paradigm of MLLMs.</b>
            Instead of relying on LLMs or external modules for localization,
            Groma exploits the spatial under-
            standing capability of the visual
            tokenizer. This perceive-then-understand design also resembles human vision process.
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered ">
      <div class="column is-four-fifths ">
        <img src="./static/images/paradigm.jpg" />
        <div class="is-size-6 has-text-justified">
          A conceptual overview of different grounded MLLMs. (a) LLM for localization (e.g., Kosmos-2, Shikra); (b) External
          modules for localization (e.g., Lisa); and (c) Visual tokenier for localization (Ours).
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <div class="content has-text-justified">
          <p>
            <b>Framework.</b> Groma encodes the image into both global image tokens and local region tokens.
            Specifically, a general-purpose region proposer is introduced to discover regions of interest,
            followed by a light-weight encoder for region tokenization.
            By integrating region tokens into user instructions and model responses, Groma unlocks the
            referring and grounding abilities.
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered ">
      <div class="column is-four-fifths ">
        <img src="./static/images/framework.jpg" />
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <div class="content has-text-justified">
          <p>
            <b>Highlights:</b><br>
            <ul>
              <li>
                <b>Balancing computation and input resolution.</b> High-resolution images are critical for accurate localization,
                but processing such images is computationally intensive for LLMs. We address this issue by shifting localization to image tokenization.
                This allows us to utilize high-resolution image inputs for the image tokenizer while downsampling the image tokens for the LLM,
                which saves computation without sacrificing localization accuracy.
              </li>
              <li>
                <b>Decoupled design for specialised training.</b> To obtain robust and precise localization capability, we pretrain Groma on large-scale detection data.
                Thanks to the decoupled design of perception and cognition within Groma, we circumvent the need to involve the LLM during detection pretraining.
                This allows Groma to benefit from pretraining on millions of bounding box annotations â€” a task that would be computationally prohibitive for classic MLLMs.
              </li>
              <li>
                <b>Unified interface for referring and grounding.</b> Referring and grounding are just like two sides of a coin -
                although different in task forms, they demand the same type of knowledge, i.e., localized understanding.
                Therefore, instead of having separate designs for referring and grounding, Groma seamlessly unifies the two capabilities with region tokens.
              </li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <h2 class="title is-3">Data</h2>
        <div class="content has-text-justified">
          <p>
            <b>Groma Instruct.</b> We curate 30k visually grounded conversations for instruction finetuning,
            which is the first grounded chat dataset constructed with both visual and textual prompts, leveraging the powerful GPT-4V for data generation.
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered ">
      <div class="column is-four-fifths ">
        <img src="./static/images/data.jpg" />
<!--        <div class="is-size-6 has-text-justified">-->
<!--          The top part demonstrates the image input with visual prompts and contextual text input to GPT-4V.-->
<!--          The bottom part is the grounded conversations generated by GPT-4V.-->
<!--        </div>-->
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-5">Referring Expression Comprehension</h2>
        <img src="./static/images/vis1.jpg"/>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-5">Region Description</h2>
        <img src="./static/images/vis2.jpg"/>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-5">Referential Dialogue</h2>
        <img src="./static/images/vis3.jpg"/>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-5">Grounded Chat</h2>
        <img src="./static/images/vis4.jpg"/>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Results</h2>
        <img src="./static/images/refcoco_results.png"/>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{ma2024groma,
      title={Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models},
      author={Chuofan Ma and Yi Jiang and Jiannan Wu and Zehuan Yuan and Xiaojuan Qi},
      year={2024},
      eprint={2404.13013},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
    }
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
<!--    <div class="content has-text-centered">-->
<!--      <a class="icon-link"-->
<!--         href="./static/videos/nerfies_paper.pdf">-->
<!--        <i class="fas fa-file-pdf"></i>-->
<!--      </a>-->
<!--      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
<!--        <i class="fab fa-github"></i>-->
<!--      </a>-->
<!--    </div>-->
    <div class="columns is-centered">
      <a href="https://github.com/nerfies/nerfies.github.io">Website template</a>
<!--      <div class="column is-8">-->
<!--        <div class="content">-->
<!--          <p>-->
<!--            This website is licensed under a <a rel="license"-->
<!--                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative-->
<!--            Commons Attribution-ShareAlike 4.0 International License</a>.-->
<!--          </p>-->
<!--          <p>-->
<!--            This means you are free to borrow the <a-->
<!--              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,-->
<!--            we just ask that you link back to this page in the footer.-->
<!--            Please remember to remove the analytics code included in the header of the website which-->
<!--            you do not want on your website.-->
<!--          </p>-->
<!--        </div>-->
      </div>
    </div>
  </div>
</footer>

</body>
</html>
